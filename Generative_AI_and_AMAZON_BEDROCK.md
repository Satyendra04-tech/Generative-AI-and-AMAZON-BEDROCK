# What is Gen AI?  
* A subset of deep learning.  
* Used to generate new data that is similar to the data it is trained on.
* We feed a lot of unlabeled data to the foundation model (FM) and based on the data the FM does a brod range of general task.  
* Creating a FM is costly.  

## Large Language Model (LLM)
1. Type of AI designed to generate coherent human like text.
2. Eg - ChatGPT / OPEN AI  
3. Trained on large amount of data.  
4. Very big models.  
5. Can perform any wide range of language related task such as Translation, summarization, question answering etc.  

## PROMPT
We usually interact with the LLM by giving a PROMPT. A PROMPT is generally a question. Then LLM will collect all the training data which was fed to generate new content.  

We never get the exact same answer for a particular prompt.  

The Prompt can be text or any image.  

## Diffusion Model  
1. Forward Diffusion process = We keep on adding noise to the picture until it disappears.  
2. Reverse Diffusion Process = We give the noise as a prompt and ask the AI to create a image out of it.  

# Amazon Bedrock  
1. Used to build GEN AI applications.  
2. Fully mananged service, no servers for us to manage.  
3. Keep control of the data that we used to train the model.  
4. Pay per use pricing model.  
5. It gives access to wide range of FMs.  
6. It makes the copy of th FM which is only available to us which we can further fine tune with our own data.  
7. Our data won't be send back to the FM providers so they won't be able to train using our data.  

# Amazon Bedrock- Base foundation model  
1. How to choose foundation model - Model types, performance requirements, capabilities, constraints and compliance.  
2. **Amazon Titan** = High performing FM from AWS.  
    i. Can do images, texts and multimodel choices.  
    ii. Can be customised with our own data. We can fine tune it.    
3. Smaller models are more cost effective but they usually know less thing.  

![Example - Amazon Titan vs. Llama vs. Claude vs. Stable Diffusion](./Photos/Amazon%20Bedrock-%20Base%20foundation%20model.png)  
  
4. Amazon is cheapest FM on Amazon Bedrock.  
5. META is open source FM on Amazon Bedrock.  
6. STABILITY AI is used for creating images using stable diffusion.  
7. We can use compare mode to see the difference in output (after giving a prompt) of 2 different FM.  
8. We can customise our models also for fine tuining.  
            a. We will take one of the available model and fine tune it with our own data.
            b. We can create a model, test a custom model and use a custom model. 
            c. We have 2 options = 1. Create fine tuning job = It's a one time thing.
                                    2. Create continued pre training job = it will keep on improving with time.

# Fine tuning a Model  
1. We are going to adapt a copy of the FM with our own data.  
2. Training data is must.  
        * Adhere to specific format.  
        * Training data be stored in Amazon S3.

## Instruction based fine tuning
1. Improves the performance of a pre trained FM on domain specific task.  
2. Instruction based fine tuning uses <mark>labeled</mark> examples that are prompt response pairs. 
3. FM is fine-tuned with specific instructions.  

## Continued Pre-training  
1. here we continue the training of the FM.  
2. We provide <mark>unlabeled</mark> data here.  
3. Also called domain adaption fine tuning to make a model expert in specific domain.  
4. We continue to train the model as more data becomes available.  

## Single turn messaging  
1. Part of instruction based fine tuning. 
2. Here we are fine tuning on how a chatbot should be replying.  

## Multi turn messaging  
1. To provide instruction based fine tuning for a conversation. 
2. It helps the model to understand how to handle conversations with bigger context.  

## Fine tuning : good to know  
1. Re training an FM requires a higher budget.  
2. Instruction based fine tuning is usually cheaper. 
3. We must prepare the data, do the fine tuning, and evaluate the model.  
4. Running a fine tune model is also expensive.  

# Transfer Learning  
1. Reusing a pre trained model to adapt it to a new related task. 
            * Widely used for image classification.  
2. Fine tuining is a specific kind of transfer learning.  

# Evaluating a Model  
1. Automatic evaluation  
            Evaluate a model for quality control.  
            Built in task type.  
            Bring your own prompt dataset or use built in curated prompt datasets.  
            Scores are claculated automatically.  
- We will have few benchmarks datasets and for each question we will have benchmark answer. We will feed the benchmarkquestion to our AI model and our model will generate some answers. So another model know as Judge model will compare the answers generated by our AI model and benchmark answers and based on similarity, a score will be given.  
- Some benchmark datasets allows us to very quikly detect any kind of bias and potential discrimination against grp of people.  
- We can create our own benchmark datasets specific to our own business requirements.  

2. Human evaluation  
The idea is same but here instead of judge model, a team of human will evaluate the AI generated answer and benchmark answers.  

# Automated metris to evaluate an FM  
1. ROUGE = Recall-Oriented Understudy for Gisting Evaluation  
            * Evaluating automatic summarization and machine translation systems.  
            * ROUGE-N - measure the number of matching n-grams between reference and generated text.  
            * ROUGE-L - longest common subsequence between reference and generated text.  

2. BLEU= Bilingual Evaluation Understudy  
            * Evaluate the quality of generated text, especially for translations.  
            * Looks at a combination of n-grams (1, 2, 3, 4)  

3. BERTScore = Semantic similarity between generated text. Checks whether the text has similar meaning or not.  

4. User satisfaction =  gather users feedbacks and assess their satisfaction with the model responses.  
5. Average revenue per user  
6. Cross domain performance = measure the models ability to perform cross different domains tasks.  
7. Conversion rate  
8. Efficiency  

## RAG and Knowledge Base  
1. RAG = Retrieval-Agumented Genaration  
2. Allows a FM to reference a data soure outside of its training data  
3. Bedrock takes care of creating vector embeddings in the dataset of your choice based on data.  
4. We retrieve the data outside of a FM and its agumented generation because we agument the prompt with that external data.  
5. It is used when real time data is needed to be fed in FM.  

## RAG Vector Databases -Types  
1. **Amazon OpenSearch Service** - search & analytics database real time similarity queries, store millions of vector embeddings scalable index management, and fast nearest-neighbor (kNN) search capability.  
2. **Amazon DocumentDB [with MongoDB compatibility]** - NoSQL database real time similarity queries, store millions of vector embeddings.  
3. **Amazon Aurora** - relational database, proprietary on AWS.  
4. **Amazon RDS for PostgreSQL** - relational database, open-source.  
5. **Amazon Neptune** - graph database  

## Amazon Bedrock - RAG - Use Cases  
- Customer Service Chatbot  
   - Knowledge Base - products, features, specifications, troubleshooting guides, and FAQs.
   - RAG application - chatbot that can answer customer queries.  
- Legal Research and Analysis 
  - Knowledge Base - laws, regulations, case precedents, legal opinions, and expert analysis.
  - RAG Application - chatbot that can provide relevant information for specific legal queries.  
- Healthcare Question-Answering  
 - Knowledge base - diseases, treatments, clinical guidelines, research papers, patients…
 - RAG application - chatbot that can answer complex medical queries.  

 ## Gen AI Concepts - Tokenization  
 - TOkenization is concept of converting raw text into a sequence of token. And each token has an ID.  
    - Word based Tokenization = text is split into individual words.  
    - Subword Tokenization = some words can be split too (helpful for long words)

## GenAI Concepts- Context Window  
- The number of tokens an LLM can consider when generating text.  
- The larger the context window, the more information and coherence.  
- Large context windows require more memory and processing power.  

## GenAI Concepts - Embeddings  
- Create vectors (array of numerical values) out of text, images or audio.  
- Embedding models can power search applications.  
- Vectors have a high dimensionality to capture many features for one input token.  
- It is very good way to use an embeddings model to power a search application. 
![GenAI Concepts - Embeddings ](./Photos/GenAI%20Concepts%20-%20Embeddings.png)  
 
- Words that have a Semantic Relationship have Similar Embeddings  
![Words that have a Semantic Relationship have Similar Embeddings](./Photos/Words%20that%20have%20a%20Semantic%20Relationship%20have%20similar%20meaning.png)  

## Amazon Bedrock - Guardrails  
- Control the interaction between users and FMs.  
- Filter undesirable and harmful content.  
- Remove Personally Identifiable Information (PII).  
- It enhances privacy and reduce hallucinations.  

![Amazon Bedrock - Guardrails](./Photos/Amazon%20Bedrock%20-%20Guardrails.png)  

## Amazon Bedrock - Agents  
- Manage and carry out various multi-step tasks.  
- Task coordination: perform tasks in the correct order and ensure information is passed correctly between tasks.  
- Agents are configured to perform specific pre-defined <mark>action groups</mark>.  
- Leverage RAG to retrieve information when necessary.  
- Agents are very smart, they know what to access nd then automatically will know how to do it.  
- Bedrock allows something know as tracing on your agent, ad this allows us to see the list of steps that were done by the agent so we can debug in case we don't like a way an agent performed something.  

## Amazon Bedrock & CloudWatch  
- CloudWatch is a way for us to do cloud montoring.  
- Model Invocation Logging  
    - Send logs of all invocations to Amazon CloudWatch and S3. 
    - Can include text, images and embeddings.  
- CloudWatch Metrics  
    - Published metrics from Bedrock to CloudWatch.  
        - Including ContentFilteredCount, which helps to see if Guardrails are functioning.  

## Bedrock - Cost savings  
- On-Demand - great for unpredictable workloads, no long-term commitment.  
- Batch Model - provides up to 50% discounts.  
- Provisioned Throughput - (usually) not a cost-saving measure, great to “reserve” capacity.  
- Temperature, Top K, Top P - no impact on pricing.  
- Model size - usually a smaller model will be cheaper.  
- Number of Input and Output Tokens - main driver of cost.  


